---MAIN---

source(models) #imports all models as template functions with parameters, dependent on model (X, Y, hypergrid, CVfolds)
source(hyperparameters) #imports hypergrids

# 5x2 repeated cross validation

N=5 (repeats)
metric = metric (AUC, BS or PG)
create empty dataframe metric_results with columns: (dataset, repeat, fold, algorithm, metric)	
	#dataset = {GC, AC, GMSC}
	#repeat = {1:5}
	#fold = {1, 2}
	#algorithm = {LR-R, MARS, SRE,...}


for (datasets) {
	for(N repeats) {

		SEED(N)
		folds <- create 50/50 train test split 
		for(folds) {

			algorithm_1 {
				SEED(N+10*fold)
				5 fold CV hyperparameter tuning
				save("algorithm1_dataset_metric")
				retrain on full training fold if not already
				make predictions and store them
			}
			calculate metric on predictions
			metric_results add row (dataset, repeat, fold, algorithm_1, metric)

			algorithm_2 {
				SEED(N+10*fold)
				5 fold CV hyperparameter tuning
				save("algorithm2_dataset_metric")
				retrain on full training fold if not already
				make predictions and store them as predictions_{fold}_dataframe column
			}
			calculate metric on predictions
			metric_results add row (dataset, repeat, fold, algorithm_2, metric)
			.
			.
			.
			
			}
		}
	}
}
save(metric_results)

#result should look like:
---------------------------------------------------------
dataset	| repeat	| fold	| algorithm	| metric |
--------|---------------|-------|---------------|--------|
german	|1		|1	|LR-R		|x	 |
german	|1		|1	|MARS		|x	 |
german	|1		|1	|SRE		|x	 |
...

# Calculate average of metric over folds and then over repeats for each algorithm and dataset.
metric_results %>% groupby(dataset, algorithm) %>% average(metric) #calculate average